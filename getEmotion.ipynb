{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary cache directory at /var/folders/y8/y2lrp6t51wz4np0f_f4kstnh0000gq/T/matplotlib-9t282jps because the default path (/Users/arjiv_admin/.matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n",
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set cache directories for XDG and Hugging Face Hub \n",
    "# os.environ['XDG_CACHE_HOME'] = 'test/.cache'\n",
    "# os.environ['HUGGINGFACE_HUB_CACHE'] = 'test/.cache'\n",
    "from scipy.special import softmax\n",
    "\n",
    "import torch\n",
    "\n",
    "# Set device to GPU if available, otherwise use CPU\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Running on device: {}'.format(device))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from moviepy.editor import VideoFileClip, ImageSequenceClip\n",
    "\n",
    "import torch\n",
    "from facenet_pytorch import (MTCNN)\n",
    "\n",
    "from transformers import (AutoFeatureExtractor,\n",
    "                          AutoModelForImageClassification,\n",
    "                          AutoConfig)\n",
    "                             \n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "def detect_emotions(image):\n",
    "    \"\"\"\n",
    "    Detect emotions from a given image.\n",
    "    Returns a tuple of the cropped face image and a\n",
    "    dictionary of class probabilities.\n",
    "    \"\"\"\n",
    "    temporary = image.copy()\n",
    "\n",
    "    # Detect faces in the image using the MTCNN group model\n",
    "    sample = mtcnn.detect(temporary)\n",
    "    if sample[0] is not None:\n",
    "        box = sample[0][0]\n",
    "\n",
    "        # Crop the face\n",
    "        face = temporary.crop(box)\n",
    "\n",
    "        # Pre-process the face\n",
    "        inputs = extractor(images=face, return_tensors=\"pt\")\n",
    "\n",
    "        # Run the image through the model\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        # Apply softmax to the logits to get probabilities\n",
    "        probabilities = torch.nn.functional.softmax(outputs.logits,\n",
    "                                                    dim=-1)\n",
    "\n",
    "        # Retrieve the id2label attribute from the configuration\n",
    "        config = AutoConfig.from_pretrained(\n",
    "            \"trpakov/vit-face-expression\"\n",
    "        )\n",
    "        id2label = config.id2label\n",
    "\n",
    "        # Convert probabilities tensor to a Python list\n",
    "        probabilities = probabilities.detach().numpy().tolist()[0]\n",
    "\n",
    "        # Map class labels to their probabilities\n",
    "        class_probabilities = {\n",
    "            id2label[i]: prob for i, prob in enumerate(probabilities)\n",
    "        }\n",
    "\n",
    "        return face, class_probabilities\n",
    "    return None, None\n",
    "mtcnn = MTCNN(\n",
    "    margin=10,\n",
    "    thresholds=[0.6, 0.7, 0.7],\n",
    "    factor=0.709,\n",
    "    post_process=True,\n",
    "    keep_all=False,\n",
    "    device=device\n",
    ")\n",
    "    \n",
    "# Load the pre-trained model and feature extractor\n",
    "extractor = AutoFeatureExtractor.from_pretrained(\n",
    "    \"trpakov/vit-face-expression\"\n",
    ")\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    \"trpakov/vit-face-expression\"\n",
    ")\n",
    "\n",
    "# Choose a frame\n",
    "def video_prob(video_data) :    \n",
    "    skips = 2\n",
    "    reduced_video = []\n",
    "\n",
    "    for i in range(0, len(video_data), skips):\n",
    "        reduced_video.append(video_data[i])\n",
    "\n",
    "# Define a list of emotions\n",
    "    emotions = [\"angry\", \"disgust\", \"fear\", \"happy\", \"neutral\", \"sad\", \"surprise\"]\n",
    "\n",
    "\n",
    "# Create a list to hold the class probabilities for all frames\n",
    "    all_class_probabilities = []\n",
    "\n",
    "# Loop over video frames\n",
    "    for i, frame in enumerate(reduced_video):\n",
    "    # Convert frame to uint8\n",
    "        frame = frame.astype(np.uint8)\n",
    "\n",
    "    # Call detect_emotions to get face and class probabilities\n",
    "        face, class_probabilities = detect_emotions(Image.fromarray(frame))\n",
    "    \n",
    "    # If a face was found\n",
    "        if face is None:\n",
    "            class_probabilities = {emotion: None for emotion in emotions}\n",
    "        \n",
    "    # Append class probabilities to the list\n",
    "        all_class_probabilities.append(list(class_probabilities.values()))\n",
    "    all_class_probabilities = np.asarray(all_class_probabilities)\n",
    "    vid_prob = np.mean(all_class_probabilities,axis = 0)\n",
    "\n",
    "    return softmax(vid_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: segment_0000.mp4\n",
      "Processing file: segment_0001.mp4\n",
      "Processing file: segment_0002.mp4\n",
      "Processing file: segment_0003.mp4\n",
      "Processing file: segment_0004.mp4\n",
      "Processing file: segment_0005.mp4\n",
      "Processing file: segment_0006.mp4\n"
     ]
    }
   ],
   "source": [
    "vid_prob = []\n",
    "vid_emotions = [\"angry\", \"disgust\", \"fear\", \"happy\", \"neutral\", \"sad\", \"surprise\"]\n",
    "\n",
    "directory = '/Users/arjiv_admin/Desktop/Emotion_POC/Video Segments'\n",
    "files = [f for f in os.listdir(directory) if f.lower().endswith('.mp4')]\n",
    "files.sort(key=lambda f: int(f.split('_')[1].split('.')[0]))\n",
    "for filename in files:\n",
    "        print(f\"Processing file: {filename}\")\n",
    "        link = os.path.join(directory, filename)\n",
    "        clip = VideoFileClip(link)\n",
    "        vid_fps = clip.fps\n",
    "        video = clip.without_audio()\n",
    "        video_data = np.array(list(video.iter_frames()))\n",
    "        prob = video_prob(video_data)\n",
    "        vid_prob.append(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/wavlm-large were not used when initializing WavLMModel: ['encoder.pos_conv_embed.conv.weight_g', 'encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing WavLMModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing WavLMModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of WavLMModel were not initialized from the model checkpoint at microsoft/wavlm-large and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at 3loi/SER-Odyssey-Baseline-WavLM-Dominance were not used when initializing SERModel: ['ssl_model.encoder.pos_conv_embed.conv.weight_g', 'ssl_model.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing SERModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing SERModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of SERModel were not initialized from the model checkpoint at 3loi/SER-Odyssey-Baseline-WavLM-Dominance and are newly initialized: ['ssl_model.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'ssl_model.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at microsoft/wavlm-large were not used when initializing WavLMModel: ['encoder.pos_conv_embed.conv.weight_g', 'encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing WavLMModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing WavLMModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of WavLMModel were not initialized from the model checkpoint at microsoft/wavlm-large and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at 3loi/SER-Odyssey-Baseline-WavLM-Valence were not used when initializing SERModel: ['ssl_model.encoder.pos_conv_embed.conv.weight_g', 'ssl_model.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing SERModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing SERModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of SERModel were not initialized from the model checkpoint at 3loi/SER-Odyssey-Baseline-WavLM-Valence and are newly initialized: ['ssl_model.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'ssl_model.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at microsoft/wavlm-large were not used when initializing WavLMModel: ['encoder.pos_conv_embed.conv.weight_g', 'encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing WavLMModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing WavLMModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of WavLMModel were not initialized from the model checkpoint at microsoft/wavlm-large and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at 3loi/SER-Odyssey-Baseline-WavLM-Arousal were not used when initializing SERModel: ['ssl_model.encoder.pos_conv_embed.conv.weight_g', 'ssl_model.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing SERModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing SERModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of SERModel were not initialized from the model checkpoint at 3loi/SER-Odyssey-Baseline-WavLM-Arousal and are newly initialized: ['ssl_model.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'ssl_model.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: segment_0000.mp3\n",
      "Predictions: [0.3891616761684418, 0.32505497336387634, 0.44186151027679443]\n",
      "Processing file: segment_0001.mp3\n",
      "Predictions: [0.6499191522598267, 0.15794216096401215, 0.692778468132019]\n",
      "Processing file: segment_0002.mp3\n",
      "Predictions: [0.6463385820388794, 0.2735784351825714, 0.7189842462539673]\n",
      "Processing file: segment_0003.mp3\n",
      "Predictions: [0.2887377142906189, 0.24893343448638916, 0.3193511366844177]\n",
      "Processing file: segment_0004.mp3\n",
      "Predictions: [0.7332590818405151, 0.31650781631469727, 0.7603095769882202]\n",
      "Processing file: segment_0005.mp3\n",
      "Predictions: [0.12423272430896759, 0.27066493034362793, 0.1293012946844101]\n",
      "Processing file: segment_0006.mp3\n",
      "Predictions: [0.15822279453277588, 0.3611600399017334, 0.10886049270629883]\n",
      "Final Audio_pred: [[0.3891616761684418, 0.32505497336387634, 0.44186151027679443], [0.6499191522598267, 0.15794216096401215, 0.692778468132019], [0.6463385820388794, 0.2735784351825714, 0.7189842462539673], [0.2887377142906189, 0.24893343448638916, 0.3193511366844177], [0.7332590818405151, 0.31650781631469727, 0.7603095769882202], [0.12423272430896759, 0.27066493034362793, 0.1293012946844101], [0.15822279453277588, 0.3611600399017334, 0.10886049270629883]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "from transformers import AutoModelForAudioClassification\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "directory = '/Users/arjiv_admin/Desktop/Emotion_POC/Audio Segments'\n",
    "\n",
    "modelDom = AutoModelForAudioClassification.from_pretrained(\"3loi/SER-Odyssey-Baseline-WavLM-Dominance\", trust_remote_code=True)\n",
    "modelVal = AutoModelForAudioClassification.from_pretrained(\"3loi/SER-Odyssey-Baseline-WavLM-Valence\", trust_remote_code=True)\n",
    "modelAro = AutoModelForAudioClassification.from_pretrained(\"3loi/SER-Odyssey-Baseline-WavLM-Arousal\", trust_remote_code=True)\n",
    "\n",
    "Audio_pred = []\n",
    "Audio_emotions = [\"Dominance\", \"Valence\", \"Arousal\"]\n",
    "\n",
    "def Audio_emotion(model, audio_path):\n",
    "    mean = model.config.mean\n",
    "    std = model.config.std\n",
    "    \n",
    "    raw_wav, sr = librosa.load(audio_path, sr=model.config.sampling_rate)\n",
    "    \n",
    "    # normalize the audio by mean/std\n",
    "    norm_wav = (raw_wav - mean) / (std + 0.000001)\n",
    "    \n",
    "    # generate the mask\n",
    "    mask = torch.ones(1, len(norm_wav))\n",
    "    \n",
    "    # batch it (add dim)\n",
    "    wavs = torch.tensor(norm_wav).unsqueeze(0)\n",
    "    \n",
    "    output = model(wavs, mask)\n",
    "    return output.item()  # Return the scalar value\n",
    "\n",
    "files = [f for f in os.listdir(directory) if f.lower().endswith('.mp3')]\n",
    "files.sort(key=lambda f: int(f.split('_')[1].split('.')[0]))\n",
    "\n",
    "for filename in files:\n",
    "    print(f\"Processing file: {filename}\")\n",
    "    audio_path = os.path.join(directory, filename)\n",
    "    with torch.no_grad():\n",
    "        pred_dom = Audio_emotion(modelDom, audio_path)\n",
    "        pred_val = Audio_emotion(modelVal, audio_path)\n",
    "        pred_aro = Audio_emotion(modelAro, audio_path)\n",
    "\n",
    "    pred = [pred_dom, pred_val, pred_aro]\n",
    "    Audio_pred.append(pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Audio_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "\n",
    "model = whisper.load_model(\"base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(task=\"text-classification\", model=\"SamLowe/roberta-base-go_emotions\", top_k=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: segment_0000.mp3\n",
      "Processing file: segment_0001.mp3\n",
      "Processing file: segment_0002.mp3\n",
      "Processing file: segment_0003.mp3\n",
      "Processing file: segment_0004.mp3\n",
      "Processing file: segment_0005.mp3\n",
      "Processing file: segment_0006.mp3\n"
     ]
    }
   ],
   "source": [
    "directory = '/Users/arjiv_admin/Desktop/Emotion_POC/Audio Segments'\n",
    "text_emotions = []\n",
    "text_preds = []\n",
    "files = [f for f in os.listdir(directory) if f.lower().endswith('.mp3')]\n",
    "files.sort(key=lambda f: int(f.split('_')[1].split('.')[0]))\n",
    "\n",
    "for filename in files:\n",
    "    print(f\"Processing file: {filename}\")\n",
    "    audio_path = os.path.join(directory, filename)\n",
    "    result = model.transcribe(audio_path)\n",
    "    model_outputs = classifier(result[\"text\"])\n",
    "    text_emotions = [d[\"label\"] for d in model_outputs[0]]\n",
    "    preds = [d[\"score\"] for d in model_outputs[0]]\n",
    "    text_preds.append(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   text_neutral  text_annoyance  text_approval  text_desire  text_curiosity  \\\n",
      "0      0.615272        0.246925       0.099582     0.033347        0.016836   \n",
      "1      0.347312        0.318416       0.102074     0.075763        0.042557   \n",
      "2      0.380562        0.365789       0.095972     0.081652        0.054079   \n",
      "3      0.364445        0.263701       0.147930     0.128249        0.067386   \n",
      "4      0.699225        0.154372       0.061320     0.018445        0.018356   \n",
      "5      0.613640        0.283586       0.064604     0.031193        0.024691   \n",
      "6      0.941637        0.011763       0.009231     0.007006        0.006505   \n",
      "\n",
      "   text_anger  text_confusion  text_realization  text_disapproval  \\\n",
      "0    0.016201        0.010918          0.007626          0.006645   \n",
      "1    0.029315        0.019223          0.014425          0.008744   \n",
      "2    0.020931        0.017390          0.010895          0.008928   \n",
      "3    0.048579        0.045061          0.022408          0.009948   \n",
      "4    0.016992        0.014691          0.012232          0.009430   \n",
      "5    0.010713        0.005360          0.003134          0.003108   \n",
      "6    0.004037        0.003355          0.002753          0.002750   \n",
      "\n",
      "   text_disappointment  ...  audio_Dominance  audio_Valence  audio_Arousal  \\\n",
      "0             0.006145  ...         0.389162       0.325055       0.441862   \n",
      "1             0.006611  ...         0.649919       0.157942       0.692778   \n",
      "2             0.008569  ...         0.646339       0.273578       0.718984   \n",
      "3             0.009542  ...         0.288738       0.248933       0.319351   \n",
      "4             0.008650  ...         0.733259       0.316508       0.760310   \n",
      "5             0.003045  ...         0.124233       0.270665       0.129301   \n",
      "6             0.002718  ...         0.158223       0.361160       0.108860   \n",
      "\n",
      "   vid_angry  vid_disgust  vid_fear  vid_happy  vid_neutral   vid_sad  \\\n",
      "0   0.118242     0.118004  0.119126   0.133046     0.272544  0.120415   \n",
      "1   0.125071     0.120251  0.127354   0.132417     0.244145  0.125374   \n",
      "2   0.125219     0.122632  0.141960   0.136944     0.199277  0.140694   \n",
      "3   0.122263     0.120836  0.133824   0.122231     0.230376  0.148777   \n",
      "4   0.125357     0.122801  0.158524   0.143468     0.188424  0.131343   \n",
      "5   0.120173     0.122656  0.124287   0.124264     0.250039  0.138279   \n",
      "6   0.115963     0.115659  0.117099   0.116730     0.301573  0.117256   \n",
      "\n",
      "   vid_surprise  \n",
      "0      0.118621  \n",
      "1      0.125389  \n",
      "2      0.133273  \n",
      "3      0.121693  \n",
      "4      0.130083  \n",
      "5      0.120303  \n",
      "6      0.115720  \n",
      "\n",
      "[7 rows x 38 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_text = pd.DataFrame(text_preds, columns= text_emotions)\n",
    "df_audio = pd.DataFrame(Audio_pred, columns = Audio_emotions)\n",
    "df_vid = pd.DataFrame(vid_prob, columns= vid_emotions)\n",
    "df_text = df_text.add_prefix('text_')\n",
    "df_audio = df_audio.add_prefix('audio_')\n",
    "df_vid = df_vid.add_prefix('vid_')\n",
    "df = pd.concat([df_text,df_audio, df_vid], axis=1)\n",
    "\n",
    "print(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
